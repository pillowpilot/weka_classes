\documentclass[10pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage{amsmath, amsfonts, amssymb}
\usepackage[backend=biber]{biblatex}
\bibliography{biblio.bib}
\usepackage[hidelinks]{hyperref}
\usepackage{csquotes}

\usepackage{color}
\usepackage{listings}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{
  language=java,                   % sets language
  backgroundcolor=\color{white},   % choose the background color
  basicstyle=\footnotesize,        % size of fonts used for the code
  breaklines=true,                 % automatic line breaking only at whitespace
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  keywordstyle=\color{blue},       % keyword style
  stringstyle=\color{mymauve},     % string literal style
}

\author{Carlos Federico Gaona}
\title{Minería de Datos con WEKA\nNotas de Curso}
\date{}

\begin{document}
\part{Conceptos}
\part{Modelos}
\section{Modelos Lineales}
Si disponemos de atributos numéricos y buscamos predecir un atributo también numérico, entonces es natural explorar los modelos lineales. Estos modelos asumen que el atributo a predecir $y^{(k)}$ responde a la forma $w_0 + \sum_i w_ix_i^{(k)}$, es decir a un hiperplano. Como se espera la presencia de errores dentro de las mediciones o simplemente se busca aproximar la forma ``real'' de $y$ se minimiza alguna métrica de distancia entre $y^{(k)}$ y $y^{(k)'} = w_o + \sum_i w_ix_i$ como pueden ser según \cite{ruppert1980trimmed}:

\begin{itemize}
\item Minima Suma de Cuadrados
  \[
  \text{min }\sum_i (y^{(i)}-y^{(i)'})^2
  \]
\item Minimos Valores Absolutos
  \[
  \text{min }\sum_i |y^{(i)}-y^{(i)'}|
  \]
\item M, L y S Estimadores
\item Minima Suma Podada de Cuadrados (Least Trimmed Squares)
  \[
  \text{min }\sum_j (y^{(j)}-y^{(j)'})^2, \quad \text{donde} \quad \{ y^{(j)}-y^{(j)'} \} \subset \{ y^{(i)}-y^{(i)'} \}
  \]
\item Minima Media de Cuadrados
  \[
  \text{min }\text{med}_i (y^{(i)}-y^{(i)'})^2
  \]
\end{itemize}

La mayor característica de los métodos lineales es, precisamente, su linealidad. Considerando que es raro encontrar un fenómeno que se comporte linealmente, la linealidad es una desventaja. Sin embargo, esto también permite que el modelo presente un menor sesgo y se verá mas adelante que esto se suele aprovechar.

%TODO Add reference to model-tree and such...

Segun \cite{witten2011data}, tenemos que \lstinline{LinearRegression()} utiliza la minimización de la suma de los cuadrados sin embargo dispone de un método de selección de atributos basado en el Criterio de Información de Akaike (ver \ref{modelselection_aic}) que simplifica el modelo resultante y está activado por defecto.
%TODO Fix reference
%TODO Add coefficients interpretation

\section{Ejemplos}

% TODO Check if dataset.setClassIndex es necesario
\begin{lstlisting}
  // Cargamos los datos de alguna manera
  Instances dataset = ...
  // Preparamos una instancia de prueba
  Instance testInstance = ...
  
  // Creamos una instancia del modelo
  Classifier model = new SimpleLinearRegression();
  // Entrenamos con los datos de 'dataset'
  model.buildClassifier(dataset);
  
  // Podemos imprimir los detalles del modelo
  System.out.println(model);
  // Tambien podemos predecir una instancia
  double prediction = model.classifyInstance(testInstance);
\end{lstlisting}

% TODO Check Logistic()
También podemos utilizar \lstinline{LinearRegression()}, \lstinline{MultilayerPerceptron()}\footnote{Si activamos la opción \textit{-G} (GUI) podremos modificar la topología de la red.} y con algunas modificaciones \lstinline{Logistic()}.

% Ventajas
%  bajo sesgo ?
%  simples de interpretar y de entrenar
% Desventajas
%  linealidad

\section{Árboles}
Una de las maneras usuales de atacar problemas es mediante la estrategia ``divide y conquistarás'', los arboles, con su estructura recursiva, represetan fielmente esta idea. Existen multiud de variedades de árboles utilizados en minería de datos, sin embargo la idea predominante es dividir el conjunto de muestras en dos o mas subconjuntos utilizando un criterio de decisión, usualmente el valor de un atributo, y repetir esto hasta que se alcance la máxima complejidad aceptable o repetir hasta que se alcanze una predicción lo suficientemente buena.

En el caso de atributos nominales, la división es trivial y única. Para los atributos numéricos, se dispone de multitud de métodos listados en \cite{kotsiantis2006discretization} y \cite{dougherty1995supervised}. En WEKA disponemos de \lstinline{weka.filters.unsupervised.attribute.NumericToNominal} que simplemente crea una biyección entre cada valor numérico y una clase, \lstinline{weka.filters.unsupervised.attribute.Discretize} divide el atributo en clases con la misma anchura (equal-width binning) con opciones de optimización o en clases con una misma frecuencia (equal-frecuency binning) y finalmente \lstinline{weka.filters.supervised.attribute.Discretize} el cual utiliza el método MDL de Fayyad e Irani, ver \cite{irani1993multi}, que es el método por defecto utilizado en los modelos.

\subsection{\lstinline{J48()}}
El modelo \lstinline{J48()} es el mas conocido entre los árboles presentes en WEKA, este modelo es la versión libre del algoritmo C4.5. En este modelo se construye un árbol de forma voráz seleccionando el atributo que ``mejor'' divide a las muestras. El criterio de selección está basado en la ganancia de información midiendo la diferencia de entropía entre el conjunto de datos original y el mismo conjunto de datos divido por el atributo.

\[
\text{Entropía} = H = \sum_i -p_i \log_2(p_i)
\]

Veamos este ejemplo. Las muestras estan originalmente clasificados en $(0, 0, 0, 0, 1, 1, 1)$ y luego de la división segun un atributo $A$ se llega $\text{right}=(0, 0, 0, 1)$ y $\text{left}=(0, 1, 1)$. Medimos la entropía de las muestras antes de la división.

\[
H_0 = -\frac{4}{7} \log\frac{4}{7} -\frac{3}{7} \log\frac{3}{7} \approx 0.985
\]
Medimos la entropía en cada subconjunto.
\[
H_{right} = -\frac{3}{4} \log\frac{3}{4} - \frac{1}{4} \log\frac{1}{4} \approx 0.811
\]
\[
H_{left} = -\frac{1}{3} \log\frac{1}{3} - \frac{2}{3} \log\frac{2}{3} \approx 0.918
\]
Obtenemos la entropía al final de la división haciendo una suma ponderada según el tamaño relativo de los subconjuntos.
\[
H_1 = \frac{4}{7}H_{right} + \frac{3}{7}H_{left} = \frac{6}{7} \approx 0.857
\]
Finalmente la Ganancia de Información, en inglés Information Gain, es la diferencia de entropías.
\[
IG = H_0 - H_1 \approx 0.128
\]

Luego de calcular la IG para cada atributo, se selecciona el mayor valor. Y se aplica recursivamente sobre los hijos. El \lstinline{J48()} aplica otros procedimientos para simplificar aún mas el árbol resultante.

\subsection{\lstinline{RandomTree()}}

\section{Clustering}
Cuando disponemos de datos sin clasificación, tenemos un problema de clustering. En clustering se busca construir una forma de clasificar los datos según su ``similaridad''. En la práctica la ``similaridad'' entre dos muestras se mide con alguna métrica %TODO Add metrics

% TODO Add most clustering scheme are very sensible to initial conditions, therefore is common to run multiple times to try to find an global optimum
% TODO Add NP-hard O(n^f(O(k))log n)
\subsection{$k$-means}
El algoritmo de $k$-means es el algoritmo de clustering mas conocido y se han propuesto multiples modificaciones para mejorar su desempeño. % TODO Add cites
En $k$-means se inicia creando en ubicaciones aleatorias $k$ puntos los cuales serán los representantes de cada clase. Luego se itera, separando las muetras en grupos tales que una muestra pertenece a la clase representada por el punto representante mas cercano y al finalizar esto se reubican los puntos representantes en el centroide de las muestras de su clase. El algoritmo converge cuando no ocurren reasignaciones, mide la similaridad con la distancia euclidiana y busca el óptimo minimizando la suma de los cuadrados de las distancias entre las muestras y su respectivo centroide. Es muy sensible a las condiciones iniciales, es decir a las posiciones iniciales de los centroides, por ello se suele correr varias veces. La elección de la $k$ es arbitraria, puede considerarse como un parámetro mas del algoritmo y probar distintos valores o puede ser proveído por el problema, como puede ser la opinión de un experto.

Esencialmente, $k$-means es un algoritmo de optimización donde se le han definido conceptos del problema de clustering. Por ello, clustering puede ser atacado con todos los recursos ya existentes enfocados a problemas de optimización.

Una de las mejoras es $k$-means++, el cual modifica el procedimiento de inicialización de los centroides. Provee una mayor carga en la inicialización, sin embargo el tiempo total se reduce y reduce considerablemente los errores finales. Sea $D(x)$ la distancia entre la muestra $x$ y el punto representante mas cercano, entonces el algoritmo $k$-means++ es el siguiente:
\begin{enumerate}
\item Seleccionar como el primer punto representante una de las $n$ muestras con una probabilidad uniforme. Es decir, cada muestra tiene la misma probabilidad de ser selecionada.
\item De las $n-1$ muestras, seleccionar el siguiente punto representante con una probabilidad igual a $\frac{D(x_i)^2}{\sum_j D(x_j)^2}$. Así se busca penalizar la selección de puntos cercanos a puntos representantes ya seleccionados.
\item Se repite el paso anterior hasta tener $k$ puntos representantes.
\item Continuamos con el algoritmo usual $k$-means, utilizando los puntos anteriormente seleccionados.
\end{enumerate}
Notamos que los centroides inicialmente estan en las mismas posiciones que algunas muestras. Los fundamentos teoricos y datos experimentales en \cite{arthur2007k}.



\part{Evaluacion}

\part{Selección de Modelos}
\section{Principio de Parsimonia}
Tambien llamado Navaja de Ockham o, en inglés, Ockham's Razor. Es un principio según el cual ``entre varias explicaciones equivalentes, se prefiere la mas simple''. 

Cabe destacar que fueron muchas las críticas contra este principio por ``ser imprudente''. La \textit{anti navaja} de A. Einstein: ``Simple, pero no más simple.''.

\section{Maximum Log-Likelihood}
En español, maximización de verosimilitud. Dado un modelo con parametros $\theta$ y un conjunto de muestras $X$ independientes e identicamente distribuidos, la verosimilitud de los parametros dado el conjunto de muestras $\mathcal{L}(\theta|X)$ es igual a la probabilidad de obtener las muestras dado los parametros $P(X|\theta)$. Es decir, cual es la ``probabilidad'' de que los parametros $\theta$ ``expliquen'' las muestras $X$.

Luego tenemos que si maximizamos $\mathcal{L}(\theta|X)$ tendremos un conjunto de parametros $\hat{\theta}$ tal que sean o sean lo suficientemente cercanos a los parametros ``reales''\footnote{Suponiendo que el modelo ``real'' sea el mismo que el utilizado.}. Entonces tendremos que $\mathcal{L}(\theta|X) = P(X|\theta) = \prod_i P(x_i|\theta)$, sin embargo maximizar esto en la práctica es dificil por ello se busca maximizar el logaritmo de el, $\max_\theta \mathcal{L}(\theta|X) = \max_\theta \log \mathcal{L}(\theta|X) = \log \mathcal{L}(\hat{\theta}|X)= \sum_i P(x_i|\theta)$.

%TODO Add plot from matlab!!
\section{Akaike Information Criterion}\label{modelselection_aic}
El AIC se define como $AIC = 2K - 2\ln{\mathcal{L}(\hat{\theta}|X)}$, donde $K$ es la cantidad de parametros que se utilizaron en el modelo mas uno y $\mathcal{L}(\hat{\theta}|X)$ es el maximo de la función de verosimilitud. Este criterio indica que el modelo es ``mejor'' si posee un \textbf{menor} valor y es un criterio de comparación entre modelos \textbf{bajo los mismos datos}, es decir que el valor en si mismo no es significativo sino si el valor es menor o no al $AIC$ de otro modelo con el cual comparamos. La interpretación es la siguiente: el término $2K$ es la penalización, por tener signo positivo y estamos buscando un menor valor, por la cantidad de parámetros que el modelo posee ya que a mayor cantidad de parámetros \textbf{mayor varianza} y mayor posibilidad de ``overfitting'', por otra parte el término $-2 \ln{\mathcal{L}(\hat{\theta}|X)}$ favorece a una mejor calificación, por el tener un signo negativo, y está relacionado a verosimilitud del modelo para esos parámetros y esas muestras y también la un \textbf{menor sesgo}\cite{Hu07}.

%TODO Add critics

\part{Importación y Exportación}
\section{Exportación}
Para exportar un modelo hacemos uso de \lstinline{weka.core.SerializationHelper()} disponible desde la versión 3.5.5. Destacamos que el método \lstinline{write} también trabaja con flujos y que es necesario capturar las excepciones que arroja.
\begin{lstlisting}
  // Disponemos del modelo a exportar
  Classifier model = ...
  
  // Utilizamos el metodo .write
  String outputFilename = ...
  SerializationHelper.write(outputFilename, model);
\end{lstlisting}

\section{Importación}
Para importar un modelo utilizamos \lstinline{weka.core.SerializationHelper()} disponible desde la versión 3.5.5. Destacamos que el método \lstinline{read} también trabaja con flujos y que es necesario capturar las excepciones que arroja.
\begin{lstlisting}
  Classifier model = (Classifier) SerializationHelper.read(inputFilename);
\end{lstlisting}

\section{Trabajado en MATLAB}
Para trabajar con las herramientas que provee WEKA utilizamos \lstinline{javaaddpath} para agregar la ubicacion de la biblioteca\footnote{El archivo que usualmente llamado ``weka.jar''.}.
%TODO Add tested code
\printbibliography
\end{document}
